{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhijit85/FederatedRAG/blob/main/TransEE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmSOksvuoAz3"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "MRLotX9cpCWr"
      },
      "outputs": [],
      "source": [
        "# whether you are using a GPU to run this Colab\n",
        "use_gpu = True\n",
        "# whether you are using a custom GCE env to run the Colab (uses different CUDA)\n",
        "custom_GCE_env = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFbWsOOboAz5",
        "outputId": "ab36a58c-66ff-41cb-b904-a4c453b78dd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install openai\n",
        "%pip install python-dotenv\n",
        "# %pip install torch-geometric\n",
        "# %pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "# %pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "# from torch_geometric.data import InMemoryDataset, DataLoader\n",
        "# import torch_geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj9IXX0LoAz8"
      },
      "source": [
        "# Data Preparation and Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Ktbb1Ambdnxv"
      },
      "outputs": [],
      "source": [
        "class CustomDataset:\n",
        "    def __init__(self, data_path: str):\n",
        "        \"\"\"\n",
        "        Custom Dataset class for loading and processing data without PyTorch Geometric.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the dataset directory.\n",
        "        \"\"\"\n",
        "\n",
        "        #data_path = '/Users/abhi/GitHUB/FederatedRAG1/DataSets/FB15k-237'\n",
        "        # Paths to files\n",
        "        self.entity_dict_path = os.path.join(data_path, 'entities.dict')\n",
        "        self.relation_dict_path = os.path.join(data_path, 'relations.dict')\n",
        "        self.train_data_path = os.path.join(data_path, 'train.txt')\n",
        "        self.valid_data_path = os.path.join(data_path, 'valid.txt')\n",
        "        self.test_data_path = os.path.join(data_path, 'test.txt')\n",
        "\n",
        "        # Load dictionaries and datasets\n",
        "        self.entity_dict = self._read_dict(self.entity_dict_path)\n",
        "        self.relation_dict = self._read_dict(self.relation_dict_path)\n",
        "\n",
        "        self.train_data = self._read_data(self.train_data_path)\n",
        "        self.valid_data = self._read_data(self.valid_data_path)\n",
        "        self.test_data = self._read_data(self.test_data_path)\n",
        "\n",
        "        self.num_entities = len(self.entity_dict)\n",
        "        self.num_relations = len(self.relation_dict)\n",
        "\n",
        "    # def _read_dict(self, file_path):\n",
        "    #     \"\"\"Read a dictionary file mapping strings to integers.\"\"\"\n",
        "    #     with open(file_path, 'r') as f:\n",
        "    #         lines = f.readlines()\n",
        "    #     return {line.split('\\t')[0]: int(line.split('\\t')[1]) for line in lines}\n",
        "\n",
        "    def _read_dict(self, file_path: str):\n",
        "        \"\"\"\n",
        "        Read entity / relation dict.\n",
        "        Format: dict({id: entity / relation})\n",
        "        \"\"\"\n",
        "\n",
        "        element_dict = {}\n",
        "        with open(file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                id_, element = line.strip().split('\\t')\n",
        "                element_dict[element] = int(id_)\n",
        "\n",
        "        return element_dict\n",
        "\n",
        "    def _read_data(self, file_path):\n",
        "        \"\"\"Read triples data and map to indices.\"\"\"\n",
        "        with open(file_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        triples = [line.strip().split('\\t') for line in lines]\n",
        "        return [(self.entity_dict[h], self.relation_dict[r], self.entity_dict[t]) for h, r, t in triples]\n",
        "\n",
        "    def get_edge_indices_and_types(self, data):\n",
        "        \"\"\"Convert triples into edge indices and types for PyTorch tensors.\"\"\"\n",
        "        heads, relations, tails = zip(*data)\n",
        "        edge_index = torch.tensor([heads, tails], dtype=torch.long)  # Shape: (2, num_edges)\n",
        "        edge_type = torch.tensor(relations, dtype=torch.long)  # Shape: (num_edges,)\n",
        "        return edge_index, edge_type\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh7tKibqoAz9"
      },
      "source": [
        "# TransGPT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "dt4KLlz5oAz9"
      },
      "outputs": [],
      "source": [
        "class TransEEnhanced(nn.Module):\n",
        "    def __init__(self, num_entities, num_relations, embedding_dim, margin, distance_metric=\"L2\",\n",
        "                 gamma=9.0, phase_weight=1.0, modulus_weight=3.5, epsilon=1.0):\n",
        "        super(TransEEnhanced, self).__init__()\n",
        "\n",
        "        # Basic TransE embeddings\n",
        "        self.entity_modulus = nn.Embedding(num_entities, embedding_dim)\n",
        "        self.entity_phase = nn.Embedding(num_entities, embedding_dim)\n",
        "        self.relation_modulus = nn.Embedding(num_relations, embedding_dim)\n",
        "        self.relation_phase = nn.Embedding(num_relations, embedding_dim)\n",
        "\n",
        "        # Margin and distance settings\n",
        "        self.margin = margin\n",
        "        self.distance_metric = distance_metric\n",
        "\n",
        "        # Hyperbolic and scaling settings\n",
        "        self.gamma = nn.Parameter(torch.Tensor([gamma]), requires_grad=False)\n",
        "        self.embedding_range = nn.Parameter(\n",
        "            torch.Tensor([(self.gamma.item() + epsilon) / embedding_dim]), requires_grad=False\n",
        "        )\n",
        "\n",
        "        # Weights for phase and modulus\n",
        "        self.phase_weight = phase_weight\n",
        "        self.modulus_weight = modulus_weight\n",
        "\n",
        "        # Initialization\n",
        "        nn.init.uniform_(self.entity_modulus.weight, a=-self.embedding_range.item(), b=self.embedding_range.item())\n",
        "        nn.init.uniform_(self.entity_phase.weight, a=-np.pi, b=np.pi)\n",
        "        nn.init.uniform_(self.relation_modulus.weight, a=-self.embedding_range.item(), b=self.embedding_range.item())\n",
        "        nn.init.uniform_(self.relation_phase.weight, a=-np.pi, b=np.pi)\n",
        "\n",
        "    def forward(self, head, relation, tail):\n",
        "        h_mod = self.entity_modulus(head)\n",
        "        h_phase = self.entity_phase(head)\n",
        "        r_mod = self.relation_modulus(relation)\n",
        "        r_phase = self.relation_phase(relation)\n",
        "        t_mod = self.entity_modulus(tail)\n",
        "        t_phase = self.entity_phase(tail)\n",
        "\n",
        "        # Modulus scoring: hyperbolic-inspired adjustment\n",
        "        modulus_score = torch.norm(h_mod * r_mod - t_mod, p=2, dim=-1)\n",
        "\n",
        "        # Phase scoring: advanced angular consistency\n",
        "        phase_diff = torch.abs(torch.sin((h_phase + r_phase - t_phase) / 2))\n",
        "        phase_score = torch.sum(phase_diff, dim=-1)\n",
        "\n",
        "        # Weighted combined score\n",
        "        score = self.modulus_weight * modulus_score + self.phase_weight * phase_score\n",
        "        return score\n",
        "\n",
        "    def compute_loss(self, positive_score, negative_score):\n",
        "        # Margin-based ranking loss\n",
        "        base_loss = F.relu(self.margin + positive_score - negative_score)\n",
        "\n",
        "        # Regularization terms for modulus and phase\n",
        "        modulus_regularization = torch.sum(torch.norm(self.entity_modulus.weight, p=2, dim=-1))\n",
        "        phase_regularization = torch.sum(torch.norm(self.entity_phase.weight, p=2, dim=-1))\n",
        "\n",
        "        # Total loss with regularization\n",
        "        total_loss = base_loss.mean() + 1e-5 * (modulus_regularization + phase_regularization)\n",
        "        return total_loss\n",
        "\n",
        "# Helper function to create corrupted edges\n",
        "def create_corrupted_edge_index(edge_index, edge_type, num_entities):\n",
        "    corrupt_head_or_tail = torch.randint(high=2, size=edge_type.size(),\n",
        "                                         device=edge_index.device)\n",
        "    random_entities = torch.randint(high=num_entities,\n",
        "                                     size=edge_type.size(), device=edge_index.device)\n",
        "    # corrupt when 1, otherwise regular head\n",
        "    heads = torch.where(corrupt_head_or_tail == 1, random_entities,\n",
        "                        edge_index[0, :])\n",
        "    # corrupt when 0, otherwise regular tail\n",
        "    tails = torch.where(corrupt_head_or_tail == 0, random_entities,\n",
        "                        edge_index[1, :])\n",
        "    return torch.stack([heads, tails], dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXiKs2dKoAz-"
      },
      "source": [
        "# Train Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-VlS8124rZw"
      },
      "source": [
        "## Model: TransE\n",
        "**Embeddings:**\n",
        "Each entity and relation is represented as a vector in a high-dimensional space.\n",
        "The embeddings are initialized randomly and updated during training.\n",
        "**Distance Metric:**\n",
        "TransE predicts relationships by minimizing the distance between embeddings of head + relation - tail.\n",
        "A lower distance indicates a more likely relationship."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAtTXKqh5Mdo"
      },
      "source": [
        "## How Is the LLM Used?\n",
        "1. Prediction Refinement\n",
        "After the TransE model predicts relationships (e.g., a tail entity for a given head and relation), these predictions are passed to the LLM.\n",
        "The LLM evaluates the predictions, identifies errors, and suggests corrections or more plausible results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "vscode": {
          "languageId": "ruby"
        },
        "id": "rnLn4HqKdnxx"
      },
      "outputs": [],
      "source": [
        "def train(model, data, optimizer, device, entity_dict, relation_dict, epochs=50, batch_size=256, valid_freq=5):\n",
        "    train_edge_index = data.train_edge_index.to(device)\n",
        "    train_edge_type = data.train_edge_type.to(device)\n",
        "    valid_edge_index = data.valid_edge_index.to(device)\n",
        "    valid_edge_type = data.valid_edge_type.to(device)\n",
        "\n",
        "    best_valid_score = 0\n",
        "    valid_scores = None\n",
        "    test_scores = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "        # Normalize entity embeddings (modulus only)\n",
        "        entities_modulus_norm = torch.norm(model.entity_modulus.weight.data, dim=1, keepdim=True)\n",
        "        model.entity_modulus.weight.data = model.entity_modulus.weight.data / entities_modulus_norm\n",
        "\n",
        "        # Shuffle the training data\n",
        "        num_triples = train_edge_type.size(0)\n",
        "        shuffled_indices = torch.randperm(num_triples)\n",
        "        shuffled_edge_index = train_edge_index[:, shuffled_indices]\n",
        "        shuffled_edge_type = train_edge_type[shuffled_indices]\n",
        "\n",
        "        negative_edge_index = create_corrupted_edge_index(shuffled_edge_index, shuffled_edge_type, data.num_entities)\n",
        "\n",
        "        total_loss = 0\n",
        "        total_size = 0\n",
        "\n",
        "        for batch_start in range(0, num_triples, batch_size):\n",
        "            batch_end = min(batch_start + batch_size, num_triples)\n",
        "            batch_edge_index = shuffled_edge_index[:, batch_start:batch_end]\n",
        "            batch_negative_edge_index = negative_edge_index[:, batch_start:batch_end]\n",
        "            batch_edge_type = shuffled_edge_type[batch_start:batch_end]\n",
        "\n",
        "            # Compute positive and negative scores for TransEEnhanced\n",
        "            positive_score = model(batch_edge_index[0], batch_edge_type, batch_edge_index[1])\n",
        "            negative_score = model(batch_negative_edge_index[0], batch_edge_type, batch_negative_edge_index[1])\n",
        "\n",
        "            # Compute loss using TransEEnhanced's loss function\n",
        "            loss = model.compute_loss(positive_score, negative_score)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * (batch_end - batch_start)\n",
        "            total_size += batch_end - batch_start\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / total_size:.4f}\")\n",
        "\n",
        "        # Validation at regular intervals\n",
        "        if (epoch + 1) % valid_freq == 0:\n",
        "            mrr_score, mr_score, hits_at_10 = evaluate_model(\n",
        "                model, valid_edge_index, valid_edge_type, data.num_entities, device\n",
        "            )\n",
        "            print(f\"Validation score: MRR = {mrr_score:.4f}, MR = {mr_score:.4f}, Hits@10 = {hits_at_10:.4f}\")\n",
        "\n",
        "            # Track best validation score\n",
        "            if mrr_score > best_valid_score:\n",
        "                best_valid_score = mrr_score\n",
        "                test_mrr, test_mr, test_hits_at_10 = evaluate_model(\n",
        "                    model, data.test_edge_index.to(device), data.test_edge_type.to(device), data.num_entities, device\n",
        "                )\n",
        "                test_scores = (test_mrr, test_mr, test_hits_at_10)\n",
        "\n",
        "    print(f\"Test scores from the best model (MMR, MR, Hits@10): {test_scores}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyUVJhNroAz-"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tzLhqtE4XK_"
      },
      "source": [
        "## Prediction:\n",
        "After training, the model can predict missing relationships by ranking possible tail entities for a given (head, relation, ?).\n",
        "Example Query:\n",
        "Input: (Steve Jobs, FounderOf, ?)\n",
        "Output: Apple (highest-ranked entity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "jY6DkHKgoAz_"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, edge_index, edge_type, num_entities, device, eval_batch_size=64):\n",
        "    model.eval()\n",
        "    num_triples = edge_type.size(0)\n",
        "    mrr_score = 0\n",
        "    mr_score = 0\n",
        "    hits_at_10 = 0\n",
        "    num_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx in range(math.ceil(num_triples / eval_batch_size)):\n",
        "            batch_start = batch_idx * eval_batch_size\n",
        "            batch_end = min((batch_idx + 1) * eval_batch_size, num_triples)\n",
        "            batch_edge_index = edge_index[:, batch_start:batch_end]\n",
        "            batch_edge_type = edge_type[batch_start:batch_end]\n",
        "            batch_size = batch_edge_type.size(0)\n",
        "\n",
        "            all_entities = torch.arange(num_entities, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "            head_repeated = batch_edge_index[0, :].reshape(-1, 1).repeat(1, num_entities)\n",
        "            relation_repeated = batch_edge_type.reshape(-1, 1).repeat(1, num_entities)\n",
        "\n",
        "            head_squeezed = head_repeated.reshape(-1)\n",
        "            relation_squeezed = relation_repeated.reshape(-1)\n",
        "            all_entities_squeezed = all_entities.reshape(-1)\n",
        "\n",
        "            entity_index_replaced_tail = torch.stack((head_squeezed, all_entities_squeezed))\n",
        "            predictions = model(entity_index_replaced_tail[0], relation_squeezed, entity_index_replaced_tail[1])\n",
        "            predictions = predictions.reshape(batch_size, -1)\n",
        "            gt = batch_edge_index[1, :].reshape(-1, 1)\n",
        "\n",
        "            mrr_score += mrr(predictions, gt)\n",
        "            mr_score += mr(predictions, gt)\n",
        "            hits_at_10 += hit_at_k(predictions, gt, device=device, k=10)\n",
        "            num_predictions += batch_size\n",
        "\n",
        "    mrr_score = mrr_score / num_predictions\n",
        "    mr_score = mr_score / num_predictions\n",
        "    hits_at_10 = hits_at_10 / num_predictions\n",
        "    return mrr_score, mr_score, hits_at_10\n",
        "\n",
        "# Metric Functions\n",
        "def mrr(predictions, gt):\n",
        "    indices = predictions.argsort()\n",
        "    return (1.0 / (indices == gt).nonzero()[:, 1].float().add(1.0)).sum().item()\n",
        "\n",
        "def mr(predictions, gt):\n",
        "    indices = predictions.argsort()\n",
        "    return ((indices == gt).nonzero()[:, 1].float().add(1.0)).sum().item()\n",
        "\n",
        "def hit_at_k(predictions, gt, device, k=10):\n",
        "    zero_tensor = torch.tensor([0], device=device)\n",
        "    one_tensor = torch.tensor([1], device=device)\n",
        "    _, indices = predictions.topk(k=k, largest=False)\n",
        "    return torch.where(indices == gt, one_tensor, zero_tensor).sum().item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a5ovUh4oAz_"
      },
      "source": [
        "# Start Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6FOxlII4jEq"
      },
      "source": [
        "## Positive Triplets:\n",
        "The dataset provides positive examples in the form of valid (head, relation, tail) triplets.\n",
        "## Negative Sampling:\n",
        "For each positive triplet, a corrupted version is generated by replacing either the head or tail with a random entity.\n",
        "## Loss Function:\n",
        "The model uses margin-based ranking loss:\n",
        "Ensures valid triplets are closer in embedding space than invalid ones by at least a predefined margin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "mpyhhj7toAz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88aa2421-124e-4f0e-f390-e0347e61eb95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 12.4975\n",
            "Epoch 2/200, Loss: 11.7215\n",
            "Epoch 3/200, Loss: 11.0980\n",
            "Epoch 4/200, Loss: 10.4526\n",
            "Epoch 5/200, Loss: 9.7241\n",
            "Epoch 6/200, Loss: 8.8747\n",
            "Epoch 7/200, Loss: 8.1494\n",
            "Epoch 8/200, Loss: 7.5547\n",
            "Epoch 9/200, Loss: 7.0488\n",
            "Epoch 10/200, Loss: 6.6479\n",
            "Validation score: MRR = 0.0212, MR = 3087.6732, Hits@10 = 0.0360\n",
            "Epoch 11/200, Loss: 6.2853\n",
            "Epoch 12/200, Loss: 5.9844\n",
            "Epoch 13/200, Loss: 5.7069\n",
            "Epoch 14/200, Loss: 5.4634\n",
            "Epoch 15/200, Loss: 5.2536\n",
            "Epoch 16/200, Loss: 5.0590\n",
            "Epoch 17/200, Loss: 4.8835\n",
            "Epoch 18/200, Loss: 4.7298\n",
            "Epoch 19/200, Loss: 4.5875\n",
            "Epoch 20/200, Loss: 4.4577\n",
            "Validation score: MRR = 0.0696, MR = 2133.4595, Hits@10 = 0.1226\n",
            "Epoch 21/200, Loss: 4.3384\n",
            "Epoch 22/200, Loss: 4.2202\n",
            "Epoch 23/200, Loss: 4.1157\n",
            "Epoch 24/200, Loss: 4.0196\n",
            "Epoch 25/200, Loss: 3.9310\n",
            "Epoch 26/200, Loss: 3.8521\n",
            "Epoch 27/200, Loss: 3.7616\n",
            "Epoch 28/200, Loss: 3.6952\n",
            "Epoch 29/200, Loss: 3.6114\n",
            "Epoch 30/200, Loss: 3.5459\n",
            "Validation score: MRR = 0.1173, MR = 1650.2186, Hits@10 = 0.2009\n",
            "Epoch 31/200, Loss: 3.4792\n",
            "Epoch 32/200, Loss: 3.4169\n",
            "Epoch 33/200, Loss: 3.3550\n",
            "Epoch 34/200, Loss: 3.2946\n",
            "Epoch 35/200, Loss: 3.2450\n",
            "Epoch 36/200, Loss: 3.1866\n",
            "Epoch 37/200, Loss: 3.1378\n",
            "Epoch 38/200, Loss: 3.0795\n",
            "Epoch 39/200, Loss: 3.0423\n",
            "Epoch 40/200, Loss: 2.9859\n",
            "Validation score: MRR = 0.1444, MR = 1277.9786, Hits@10 = 0.2488\n",
            "Epoch 41/200, Loss: 2.9429\n",
            "Epoch 42/200, Loss: 2.9037\n",
            "Epoch 43/200, Loss: 2.8549\n",
            "Epoch 44/200, Loss: 2.8128\n",
            "Epoch 45/200, Loss: 2.7743\n",
            "Epoch 46/200, Loss: 2.7323\n",
            "Epoch 47/200, Loss: 2.6896\n",
            "Epoch 48/200, Loss: 2.6530\n",
            "Epoch 49/200, Loss: 2.6173\n",
            "Epoch 50/200, Loss: 2.5764\n",
            "Validation score: MRR = 0.1606, MR = 992.4846, Hits@10 = 0.2777\n",
            "Epoch 51/200, Loss: 2.5430\n",
            "Epoch 52/200, Loss: 2.5040\n",
            "Epoch 53/200, Loss: 2.4710\n",
            "Epoch 54/200, Loss: 2.4342\n",
            "Epoch 55/200, Loss: 2.3972\n",
            "Epoch 56/200, Loss: 2.3719\n",
            "Epoch 57/200, Loss: 2.3329\n",
            "Epoch 58/200, Loss: 2.2985\n",
            "Epoch 59/200, Loss: 2.2676\n",
            "Epoch 60/200, Loss: 2.2365\n",
            "Validation score: MRR = 0.1730, MR = 768.4432, Hits@10 = 0.3030\n",
            "Epoch 61/200, Loss: 2.2076\n",
            "Epoch 62/200, Loss: 2.1768\n",
            "Epoch 63/200, Loss: 2.1491\n",
            "Epoch 64/200, Loss: 2.1148\n",
            "Epoch 65/200, Loss: 2.0874\n",
            "Epoch 66/200, Loss: 2.0598\n",
            "Epoch 67/200, Loss: 2.0305\n",
            "Epoch 68/200, Loss: 2.0040\n",
            "Epoch 69/200, Loss: 1.9721\n",
            "Epoch 70/200, Loss: 1.9446\n",
            "Validation score: MRR = 0.1883, MR = 600.4833, Hits@10 = 0.3286\n",
            "Epoch 71/200, Loss: 1.9233\n",
            "Epoch 72/200, Loss: 1.8873\n",
            "Epoch 73/200, Loss: 1.8693\n",
            "Epoch 74/200, Loss: 1.8449\n",
            "Epoch 75/200, Loss: 1.8209\n",
            "Epoch 76/200, Loss: 1.7955\n",
            "Epoch 77/200, Loss: 1.7700\n",
            "Epoch 78/200, Loss: 1.7455\n",
            "Epoch 79/200, Loss: 1.7220\n",
            "Epoch 80/200, Loss: 1.6935\n",
            "Validation score: MRR = 0.2054, MR = 467.0764, Hits@10 = 0.3512\n",
            "Epoch 81/200, Loss: 1.6786\n",
            "Epoch 82/200, Loss: 1.6555\n",
            "Epoch 83/200, Loss: 1.6338\n",
            "Epoch 84/200, Loss: 1.6125\n",
            "Epoch 85/200, Loss: 1.5898\n",
            "Epoch 86/200, Loss: 1.5726\n",
            "Epoch 87/200, Loss: 1.5501\n",
            "Epoch 88/200, Loss: 1.5260\n",
            "Epoch 89/200, Loss: 1.5084\n",
            "Epoch 90/200, Loss: 1.4861\n",
            "Validation score: MRR = 0.2206, MR = 372.4109, Hits@10 = 0.3726\n",
            "Epoch 91/200, Loss: 1.4684\n",
            "Epoch 92/200, Loss: 1.4498\n",
            "Epoch 93/200, Loss: 1.4339\n",
            "Epoch 94/200, Loss: 1.4167\n",
            "Epoch 95/200, Loss: 1.3995\n",
            "Epoch 96/200, Loss: 1.3785\n",
            "Epoch 97/200, Loss: 1.3657\n",
            "Epoch 98/200, Loss: 1.3500\n",
            "Epoch 99/200, Loss: 1.3334\n",
            "Epoch 100/200, Loss: 1.3146\n",
            "Validation score: MRR = 0.2320, MR = 299.0360, Hits@10 = 0.3909\n",
            "Epoch 101/200, Loss: 1.2999\n",
            "Epoch 102/200, Loss: 1.2832\n",
            "Epoch 103/200, Loss: 1.2696\n",
            "Epoch 104/200, Loss: 1.2604\n",
            "Epoch 105/200, Loss: 1.2396\n",
            "Epoch 106/200, Loss: 1.2280\n",
            "Epoch 107/200, Loss: 1.2126\n",
            "Epoch 108/200, Loss: 1.1963\n",
            "Epoch 109/200, Loss: 1.1846\n",
            "Epoch 110/200, Loss: 1.1765\n",
            "Validation score: MRR = 0.2432, MR = 255.4758, Hits@10 = 0.4055\n",
            "Epoch 111/200, Loss: 1.1709\n",
            "Epoch 112/200, Loss: 1.1532\n",
            "Epoch 113/200, Loss: 1.1426\n",
            "Epoch 114/200, Loss: 1.1296\n",
            "Epoch 115/200, Loss: 1.1193\n",
            "Epoch 116/200, Loss: 1.1085\n",
            "Epoch 117/200, Loss: 1.0938\n",
            "Epoch 118/200, Loss: 1.0900\n",
            "Epoch 119/200, Loss: 1.0785\n",
            "Epoch 120/200, Loss: 1.0633\n",
            "Validation score: MRR = 0.2532, MR = 219.6967, Hits@10 = 0.4217\n",
            "Epoch 121/200, Loss: 1.0621\n",
            "Epoch 122/200, Loss: 1.0557\n",
            "Epoch 123/200, Loss: 1.0437\n",
            "Epoch 124/200, Loss: 1.0327\n",
            "Epoch 125/200, Loss: 1.0299\n",
            "Epoch 126/200, Loss: 1.0246\n",
            "Epoch 127/200, Loss: 1.0063\n",
            "Epoch 128/200, Loss: 1.0040\n",
            "Epoch 129/200, Loss: 0.9967\n",
            "Epoch 130/200, Loss: 0.9917\n",
            "Validation score: MRR = 0.2607, MR = 196.7951, Hits@10 = 0.4278\n",
            "Epoch 131/200, Loss: 0.9834\n",
            "Epoch 132/200, Loss: 0.9731\n",
            "Epoch 133/200, Loss: 0.9685\n",
            "Epoch 134/200, Loss: 0.9676\n",
            "Epoch 135/200, Loss: 0.9590\n",
            "Epoch 136/200, Loss: 0.9557\n",
            "Epoch 137/200, Loss: 0.9507\n",
            "Epoch 138/200, Loss: 0.9404\n",
            "Epoch 139/200, Loss: 0.9424\n",
            "Epoch 140/200, Loss: 0.9311\n",
            "Validation score: MRR = 0.2634, MR = 187.7748, Hits@10 = 0.4364\n",
            "Epoch 141/200, Loss: 0.9286\n",
            "Epoch 142/200, Loss: 0.9215\n",
            "Epoch 143/200, Loss: 0.9163\n",
            "Epoch 144/200, Loss: 0.9163\n",
            "Epoch 145/200, Loss: 0.9099\n",
            "Epoch 146/200, Loss: 0.9083\n",
            "Epoch 147/200, Loss: 0.9020\n",
            "Epoch 148/200, Loss: 0.9018\n",
            "Epoch 149/200, Loss: 0.9013\n",
            "Epoch 150/200, Loss: 0.8977\n",
            "Validation score: MRR = 0.2664, MR = 179.2486, Hits@10 = 0.4403\n",
            "Epoch 151/200, Loss: 0.8869\n",
            "Epoch 152/200, Loss: 0.8901\n",
            "Epoch 153/200, Loss: 0.8896\n",
            "Epoch 154/200, Loss: 0.8844\n",
            "Epoch 155/200, Loss: 0.8763\n",
            "Epoch 156/200, Loss: 0.8723\n",
            "Epoch 157/200, Loss: 0.8768\n",
            "Epoch 158/200, Loss: 0.8694\n",
            "Epoch 159/200, Loss: 0.8658\n",
            "Epoch 160/200, Loss: 0.8656\n",
            "Validation score: MRR = 0.2691, MR = 171.9992, Hits@10 = 0.4401\n",
            "Epoch 161/200, Loss: 0.8630\n",
            "Epoch 162/200, Loss: 0.8629\n",
            "Epoch 163/200, Loss: 0.8639\n",
            "Epoch 164/200, Loss: 0.8576\n",
            "Epoch 165/200, Loss: 0.8571\n",
            "Epoch 166/200, Loss: 0.8564\n",
            "Epoch 167/200, Loss: 0.8593\n",
            "Epoch 168/200, Loss: 0.8511\n",
            "Epoch 169/200, Loss: 0.8530\n",
            "Epoch 170/200, Loss: 0.8501\n",
            "Validation score: MRR = 0.2727, MR = 167.4528, Hits@10 = 0.4463\n",
            "Epoch 171/200, Loss: 0.8441\n",
            "Epoch 172/200, Loss: 0.8427\n",
            "Epoch 173/200, Loss: 0.8423\n",
            "Epoch 174/200, Loss: 0.8433\n",
            "Epoch 175/200, Loss: 0.8396\n",
            "Epoch 176/200, Loss: 0.8334\n",
            "Epoch 177/200, Loss: 0.8360\n",
            "Epoch 178/200, Loss: 0.8401\n",
            "Epoch 179/200, Loss: 0.8318\n",
            "Epoch 180/200, Loss: 0.8383\n",
            "Validation score: MRR = 0.2725, MR = 165.0276, Hits@10 = 0.4409\n",
            "Epoch 181/200, Loss: 0.8329\n",
            "Epoch 182/200, Loss: 0.8341\n",
            "Epoch 183/200, Loss: 0.8311\n",
            "Epoch 184/200, Loss: 0.8304\n",
            "Epoch 185/200, Loss: 0.8304\n",
            "Epoch 186/200, Loss: 0.8281\n",
            "Epoch 187/200, Loss: 0.8316\n",
            "Epoch 188/200, Loss: 0.8322\n",
            "Epoch 189/200, Loss: 0.8320\n",
            "Epoch 190/200, Loss: 0.8291\n",
            "Validation score: MRR = 0.2723, MR = 163.8306, Hits@10 = 0.4465\n",
            "Epoch 191/200, Loss: 0.8278\n",
            "Epoch 192/200, Loss: 0.8278\n",
            "Epoch 193/200, Loss: 0.8255\n",
            "Epoch 194/200, Loss: 0.8237\n",
            "Epoch 195/200, Loss: 0.8238\n",
            "Epoch 196/200, Loss: 0.8260\n",
            "Epoch 197/200, Loss: 0.8188\n",
            "Epoch 198/200, Loss: 0.8139\n",
            "Epoch 199/200, Loss: 0.8194\n",
            "Epoch 200/200, Loss: 0.8208\n",
            "Validation score: MRR = 0.2732, MR = 163.5773, Hits@10 = 0.4444\n",
            "Test scores from the best model (MMR, MR, Hits@10): (0.266797123376568, 166.1831818626014, 0.4362845695299521)\n"
          ]
        }
      ],
      "source": [
        "lr =  0.001 #0.003\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    epochs = 200 #80\n",
        "    valid_freq = 10\n",
        "else:\n",
        "    epochs = 10\n",
        "    valid_freq = 10\n",
        "\n",
        "device = torch.device('cuda' if use_gpu else 'cpu')\n",
        "\n",
        "# Load dataset using CustomDataset class\n",
        "data_path = '/content/sample_data'\n",
        "dataset = CustomDataset(data_path)\n",
        "data = dataset\n",
        "\n",
        "# Extract edge indices and types\n",
        "train_edge_index, train_edge_type = dataset.get_edge_indices_and_types(dataset.train_data)\n",
        "valid_edge_index, valid_edge_type = dataset.get_edge_indices_and_types(dataset.valid_data)\n",
        "test_edge_index, test_edge_type = dataset.get_edge_indices_and_types(dataset.test_data)\n",
        "\n",
        "model = TransEEnhanced(data.num_entities, data.num_relations, embedding_dim=200, margin=9.0).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "entity_dict = dataset.entity_dict  # Load entity dictionary from dataset\n",
        "relation_dict = dataset.relation_dict  # Load relation dictionary from dataset\n",
        "\n",
        "# Create a data object with the extracted edge indices and types\n",
        "data.train_edge_index = train_edge_index\n",
        "data.train_edge_type = train_edge_type\n",
        "data.valid_edge_index = valid_edge_index\n",
        "data.valid_edge_type = valid_edge_type\n",
        "data.test_edge_index = test_edge_index\n",
        "data.test_edge_type = test_edge_type\n",
        "\n",
        "# Training\n",
        "train(\n",
        "    model=model,\n",
        "    data=data,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    entity_dict=entity_dict,\n",
        "    relation_dict=relation_dict,\n",
        "    epochs=epochs,  # Use the epochs variable defined earlier\n",
        "    batch_size=256,\n",
        "    valid_freq=valid_freq  # Use the valid_freq variable defined earlier\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# # Check if CUDA is available\n",
        "# if torch.cuda.is_available():\n",
        "#     # Get the device properties\n",
        "#     device_properties = torch.cuda.get_device_properties(0)  # 0 for the first GPU\n",
        "\n",
        "#     # Get total memory in bytes\n",
        "#     total_memory = device_properties.total_memory\n",
        "\n",
        "#     # Get allocated memory in bytes\n",
        "#     allocated_memory = torch.cuda.memory_allocated(0)\n",
        "\n",
        "#     # Get reserved memory in bytes\n",
        "#     reserved_memory = torch.cuda.memory_reserved(0)\n",
        "\n",
        "#     # Calculate free memory in bytes\n",
        "#     free_memory = total_memory - allocated_memory - reserved_memory\n",
        "\n",
        "#     # Print the results in GB\n",
        "#     print(f\"Total CUDA memory: {total_memory / (1024**3):.2f} GB\")\n",
        "#     print(f\"Allocated CUDA memory: {allocated_memory / (1024**3):.2f} GB\")\n",
        "#     print(f\"Reserved CUDA memory: {reserved_memory / (1024**3):.2f} GB\")\n",
        "#     print(f\"Free CUDA memory: {free_memory / (1024**3):.2f} GB\")\n",
        "\n",
        "# else:\n",
        "#     print(\"CUDA is not available.\")"
      ],
      "metadata": {
        "id": "I0CTDsvPvsCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs_oJCzJ3am6"
      },
      "source": [
        "# Example Workflow:\n",
        "## Input:\n",
        "\n",
        "**Dataset**: (Barack Obama, PresidentOf, United States), (Elon Musk, FounderOf, Tesla).\n",
        "**Embedding Initialization**:\n",
        "\n",
        "**Entities**: Barack Obama, United States, Elon Musk, Tesla.\n",
        "**Relations**: PresidentOf, FounderOf.\n",
        "**Training:**\n",
        "\n",
        "**Positive Triplets**: (Barack Obama, PresidentOf, United States).\n",
        "**Negative Sampling**: (Barack Obama, PresidentOf, RandomEntity).\n",
        "Evaluation:\n",
        "\n",
        "Metrics like **MRR, MR,** and **Hits@10** are computed during validation to measure the modelâ€™s performance.\n",
        "Prediction:\n",
        "\n",
        "**Query**: *(Elon Musk, FounderOf, ?)*\n",
        "**Prediction**: Tesla (most likely tail).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}